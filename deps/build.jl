using CUDAapi
using CUDAdrv
using LLVM

# FIXME: replace with an additional log level when we depend on 0.7+
macro trace(ex...)
    esc(:(@debug $(ex...)))
end


## auxiliary routines

function llvm_support(version)
    @debug("Using LLVM $version")

    InitializeAllTargets()
    haskey(targets(), "nvptx") ||
        error("Your LLVM does not support the NVPTX back-end. Fix this, and rebuild LLVM.jl and CUDAnative.jl")

    target_support = sort(collect(CUDAapi.devices_for_llvm(version)))

    ptx_support = CUDAapi.isas_for_llvm(version)
    if VERSION >= v"0.7.0-DEV.1959"
        # JuliaLang/julia#23817 includes a patch with PTX ISA 6.0 support
        push!(ptx_support, v"6.0")
    end
    ptx_support = sort(collect(ptx_support))

    @trace("LLVM support", targets=target_support, isas=ptx_support)
    return target_support, ptx_support
end

function cuda_support(driver_version, toolkit_version)
    @debug("Using CUDA driver $driver_version and toolkit $toolkit_version")

    # the toolkit version as reported contains major.minor.patch,
    # but the version number returned by libcuda is only major.minor.
    toolkit_version = VersionNumber(toolkit_version.major, toolkit_version.minor)
    if toolkit_version > driver_version
        error("CUDA $(toolkit_version.major).$(toolkit_version.minor) is not supported by ",
              "your driver (which supports up to $(driver_version.major).$(driver_version.minor)")
    end

    driver_target_support = CUDAapi.devices_for_cuda(driver_version)
    toolkit_target_support = CUDAapi.devices_for_cuda(toolkit_version)
    target_support = sort(collect(driver_target_support ∩ toolkit_target_support))

    driver_ptx_support = CUDAapi.isas_for_cuda(driver_version)
    toolkit_ptx_support = CUDAapi.isas_for_cuda(toolkit_version)
    ptx_support = sort(collect(driver_ptx_support ∩ toolkit_ptx_support))

    @trace("CUDA support", targets=target_support, isas=ptx_support)
    return target_support, ptx_support
end


## main

const config_path = joinpath(@__DIR__, "ext.jl")
const previous_config_path = config_path * ".bak"

function write_ext(config)
    open(config_path, "w") do io
        println(io, "# autogenerated file, do not edit")
        for (key,val) in config
            println(io, "const $key = $(repr(val))")
        end
    end
end

function main()
    ispath(config_path) && mv(config_path, previous_config_path; remove_destination=true)
    config = Dict{Symbol,Any}(:configured => false)
    write_ext(config)


    ## gather info

    ### LLVM.jl

    LLVM.libllvm_system && error("CUDAnative.jl requires LLVM.jl to be built against Julia's LLVM library, not a system-provided one")

    config[:llvm_version] = LLVM.version()
    llvm_targets, llvm_isas = llvm_support(config[:llvm_version])

    ### julia

    config[:julia_version] = VERSION

    config[:julia_llvm_version] = Base.libllvm_version
    if config[:julia_llvm_version] != config[:llvm_version]
        error("LLVM $(config[:llvm_version]) incompatible with Julia's LLVM $(config[:julia_llvm_version])")
    end

    ### CUDA

    toolkit_dirs = find_toolkit()
    config[:cuda_toolkit_version] = find_toolkit_version(toolkit_dirs)

    config[:cuda_driver_version] = CUDAdrv.version()
    cuda_targets, cuda_isas = cuda_support(config[:cuda_driver_version], config[:cuda_toolkit_version])

    config[:target_support] = sort(collect(llvm_targets ∩ cuda_targets))
    isempty(config[:target_support]) && error("Your toolchain does not support any device target")

    config[:ptx_support] = sort(collect(llvm_isas ∩ cuda_isas))
    isempty(config[:target_support]) && error("Your toolchain does not support any PTX ISA")

    @debug("CUDAnative support", targets=config[:target_support], isas=config[:ptx_support])

    # discover other CUDA toolkit artifacts
    config[:libdevice] = find_libdevice(config[:target_support], toolkit_dirs)
    config[:libdevice] == nothing && error("Available CUDA toolchain does not provide libdevice")
    config[:cuobjdump] = find_cuda_binary("cuobjdump", toolkit_dirs)
    config[:cuobjdump] == nothing && error("Available CUDA toolchain does not provide cuobjdump")
    config[:ptxas] = find_cuda_binary("ptxas", toolkit_dirs)
    config[:ptxas] == nothing && error("Available CUDA toolchain does not provide ptxas")


    ## (re)generate ext.jl

    function globals(mod)
        all_names = names(mod, all=true)
        filter(name-> !any(name .== [nameof(mod), Symbol("#eval"), :eval]), all_names)
    end

    if isfile(previous_config_path)
        @debug("Checking validity of existing ext.jl...")
        @eval module Previous; include($previous_config_path); end
        previous_config = Dict{Symbol,Any}(name => getfield(Previous, name)
                                           for name in globals(Previous))

        if config == previous_config
            info("CUDAnative.jl has already been built for this toolchain, no need to rebuild")
            mv(previous_config_path, config_path; remove_destination=true)
            return
        end
    end

    config[:configured] = true
    write_ext(config)

    return
end

main()
